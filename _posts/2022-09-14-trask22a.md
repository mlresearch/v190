---
title: 'Hierarchical partition of unity networks: fast multilevel training'
abstract: 'We present a probabilistic mixture of experts framework to perform nonparametric
  piecewise polynomial approximation without the need for an underlying mesh partitioning
  space. Deep neural networks traditionally used for classification provide a means
  of localizing polynomial approximation, and the probabilistic formulation admits
  a trivially parallelizable expectation maximization (EM) strategy. We then introduce
  a hierarchical architecture whose EM loss naturally decomposes into coarse and fine
  scale terms and small decoupled least squares problems. We exploit this hierarchical
  structure to formulate a V-cycle multigrid-inspired training algorithm. A suite
  of benchmarks demonstrate the ability of the scheme to: realize for smooth data
  algebraic convergence with respect to number of partitions, exponential convergence
  with respect to polynomial order; exactly reproduce piecewise polynomial functions;
  and demonstrate through an application to data-driven semiconductor modeling the
  ability to accurately treat data spanning several orders of magnitude.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: trask22a
month: 0
tex_title: 'Hierarchical partition of unity networks: fast multilevel training'
firstpage: 271
lastpage: 286
page: 271-286
order: 271
cycles: false
bibtex_author: Trask, Nathaniel and Henriksen, Amelia and Martinez, Carianne and Cyr,
  Eric
author:
- given: Nathaniel
  family: Trask
- given: Amelia
  family: Henriksen
- given: Carianne
  family: Martinez
- given: Eric
  family: Cyr
date: 2022-09-14
address:
container-title: Proceedings of Mathematical and Scientific Machine Learning
volume: '190'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 9
  - 14
pdf: https://proceedings.mlr.press/v190/trask22a/trask22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
