---
title: Natural Compression for Distributed Deep Learning
abstract: 'Modern deep learning models are often trained in parallel over a collection
  of distributed machines to reduce training time. In such settings, communication
  of model updates among machines becomes a significant performance bottleneck and
  various lossy update compression techniques have been proposed to alleviate this
  problem. In this work, we introduce a new, simple yet theoretically and practically
  effective compression technique: {\em natural compression ($C_{\text{nat}}$)}. Our
  technique is applied individually to all entries of the to-be-compressed update
  vector and works by randomized rounding to the nearest (negative or positive) power
  of two, which can be computed in a “natural” way by ignoring the mantissa. We show
  that compared to no compression, $C_{\text{nat}}$ increases the second moment of
  the compressed vector by not more than the tiny factor $\frac{9}{8}$, which means
  that the effect of $C_{\text{nat}}$ on the convergence speed of popular training
  algorithms, such as distributed SGD, is negligible. However, the communications
  savings enabled by $C_{\text{nat}}$ are substantial, leading to {\em $3$-$4\times$
  improvement in overall theoretical running time}. For applications requiring more
  aggressive compression, we generalize $C_{\text{nat}}$ to {\em natural dithering},
  which we prove is {\em exponentially better} than the common random dithering technique.
  Our compression operators can be used on their own or in combination with existing
  operators for a more aggressive combined effect, and offer new state-of-the-art
  both in theory and practice.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: horvoth22a
month: 0
tex_title: Natural Compression for Distributed Deep Learning
firstpage: 129
lastpage: 141
page: 129-141
order: 129
cycles: false
bibtex_author: Horv\'{o}th, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu,
  Atal Narayan and Canini, Marco and Richtarik, Peter
author:
- given: Samuel
  family: Horvóth
- given: Chen-Yu
  family: Ho
- given: Ludovit
  family: Horvath
- given: Atal Narayan
  family: Sahu
- given: Marco
  family: Canini
- given: Peter
  family: Richtarik
date: 2022-09-14
address:
container-title: Proceedings of Mathematical and Scientific Machine Learning
volume: '190'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 9
  - 14
pdf: https://proceedings.mlr.press/v190/horvoth22a/horvoth22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
